# ðŸ“ˆ Model Evaluation

## Multi-Class Metrics

A simple "Accuracy" metric is critically flawed when evaluating the Player Selection System. If a batting system has 75 Average/Poor players and 5 Excellent players, a model that simply guesses "Average" every single time will achieve high accuracy while completely failing its primary objective: identifying elite talent.

### Primary Metric: Weighted F1-Score

We utilize the **F1-Score** (harmonic mean of Precision and Recall) as our north star metric, primarily evaluating specific class outputs via a **Classification Report**.

- **Precision:** Of all the players the model labeled `Excellent`, how many were actually `Excellent`? (Minimising False Positives â€” picking a bad player).
- **Recall:** Of all the mathematically true `Excellent` players in the dataset, how many did the model correctly identify? (Minimising False Negatives â€” dropping an elite performer).
- **Weighted Average:** Since classes are imbalanced, we rely on the strictly weighted metrics across all 4 categorical tiers (`Poor`, `Average`, `Good`, `Excellent`).

## ðŸ“Š Confusion Matrices

The confusion matrix visually validates our tree splits:

- It highlights if the model is routinely confusing `Good` with `Excellent`.
- For sports analytics, a model confusing `Average` and `Poor` is acceptable. However, a model confusing `Excellent` and `Poor` indicates highly compromised feature weighting or severe data drift.

By analyzing the grid, we isolate where the Random Forest splits struggle (e.g. failing to identify low-strike-rate, high-average anchor batsmen).

## Sanity Checks (Domain Reality)

Machine Learning in sports is meaningless if it contradicts fundamental domain reality. The ultimate evaluation step in our pipeline is a semantic sanity test:

```python
# Check: do our top-rated players match known SL cricketing wisdom?
top_batsmen = batting_df[batting_df['performance_label'] == 'Excellent']['player'].value_counts()
print(top_batsmen.head(10))
```

If the output generated by the validation set ranks obscure bench warmers as `Excellent` over established players like Pathum Nissanka, Kusal Mendis, or Wanindu Hasaranga (assuming they are in form), the mathematical labels or rolling window length (`10`) must be re-calibrated.

If the output aligns with observational cricketing wisdom, we validate the model for deployment to the Streamlit App.
